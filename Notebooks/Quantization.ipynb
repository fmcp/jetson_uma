{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ow23zCFEH4v"
      },
      "source": [
        "# Ejemplo de Quantization\n",
        "---\n",
        "\n",
        "En este ejemplo vamos a ver como cambiar la representación del modelo pasando los pesos y activaciones de FP32 a INT8. De esta forma, se obtinenen dos beneficios potenciales:\n",
        "\n",
        "\n",
        "1.   Reducimos el tamaño que ocupa el modelo ya que los pesos ocupan una cuarta parte (8 bits vs 32 bits por peso).\n",
        "2.   Si el dispositivo incorpora hardware para trabajar en 8 bits, se reduce el tiempo de ejecución. Sino, se mantiene el mismo que para 32 bits.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Instalar e importar las librerías necesarias\n",
        "\n",
        "En este ejemplo vamos a trabajar con Pytorch y los modelos de torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnsq0y32D6KW",
        "outputId": "0196d39a-fd64-4277-e443-6c7e9caaef82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchinfo numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QcTQd3QwEM5j"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet18, ResNet18_Weights, resnet50, ResNet50_Weights\n",
        "from torchinfo import summary\n",
        "import torch\n",
        "import torchvision\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk6e0K0QGkOa"
      },
      "source": [
        "## 2. Definir el modelo\n",
        "\n",
        "Definimos el modelo, en este caso, usamos AlexNet pre-entrenada en ImageNet. Usamos esta red ya que es una red lineal sin conexiones residuales que producen problemas con la cuantización. Este tipo de problemas se pueden solventar cambiando algunas operaciones del modelo como se ve en este ejemplo para ResNet50 (https://github.com/zanvari/resnet50-quantization/blob/main/quantization-resnet50.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kPW7a7CGAIm",
        "outputId": "ff9a5fc9-af24-4f34-ecb9-08fe60dc3d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:05<00:00, 48.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "AlexNet                                  [1, 1000]                 --\n",
              "├─Sequential: 1-1                        [1, 256, 6, 6]            --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 55, 55]           23,296\n",
              "│    └─ReLU: 2-2                         [1, 64, 55, 55]           --\n",
              "│    └─MaxPool2d: 2-3                    [1, 64, 27, 27]           --\n",
              "│    └─Conv2d: 2-4                       [1, 192, 27, 27]          307,392\n",
              "│    └─ReLU: 2-5                         [1, 192, 27, 27]          --\n",
              "│    └─MaxPool2d: 2-6                    [1, 192, 13, 13]          --\n",
              "│    └─Conv2d: 2-7                       [1, 384, 13, 13]          663,936\n",
              "│    └─ReLU: 2-8                         [1, 384, 13, 13]          --\n",
              "│    └─Conv2d: 2-9                       [1, 256, 13, 13]          884,992\n",
              "│    └─ReLU: 2-10                        [1, 256, 13, 13]          --\n",
              "│    └─Conv2d: 2-11                      [1, 256, 13, 13]          590,080\n",
              "│    └─ReLU: 2-12                        [1, 256, 13, 13]          --\n",
              "│    └─MaxPool2d: 2-13                   [1, 256, 6, 6]            --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            --\n",
              "├─Sequential: 1-3                        [1, 1000]                 --\n",
              "│    └─Dropout: 2-14                     [1, 9216]                 --\n",
              "│    └─Linear: 2-15                      [1, 4096]                 37,752,832\n",
              "│    └─ReLU: 2-16                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-17                     [1, 4096]                 --\n",
              "│    └─Linear: 2-18                      [1, 4096]                 16,781,312\n",
              "│    └─ReLU: 2-19                        [1, 4096]                 --\n",
              "│    └─Linear: 2-20                      [1, 1000]                 4,097,000\n",
              "==========================================================================================\n",
              "Total params: 61,100,840\n",
              "Trainable params: 61,100,840\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 714.68\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 3.95\n",
              "Params size (MB): 244.40\n",
              "Estimated Total Size (MB): 248.96\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model = torchvision.models.alexnet(weights=torchvision.models.AlexNet_Weights)\n",
        "preprocessing = ResNet18_Weights.IMAGENET1K_V1.transforms()\n",
        "summary(model, input_size=(1, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEdYSwpwM2gn"
      },
      "source": [
        "## 3. Definir un data loader\n",
        "\n",
        "Una vez descargados los datos, tenemos que crear un DataLoader de Pytorch para poder usarlos con nuestro modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN8vwaeNb6tU",
        "outputId": "63d3a404-a4d5-4895-d943-8c6ecddfc511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 43691184.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar10/cifar-10-python.tar.gz to ./cifar10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "dataset = torchvision.datasets.CIFAR10(root='./cifar10', train=True, transform=preprocessing, download=True)\n",
        "train_data_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ihHzo0DO9SL"
      },
      "source": [
        "## 4. Preparar la cuantización\n",
        "\n",
        "En este ejemplo vamos a usar una 'Quantization-Aware Training' para calibrar y transformar los pesos y activaciones de FP32 a INT8. De esta forma, los pesos se adaptan al nuevo rango de representación evitando problemas de cálculos que se salen fuera de rango y obteniendo un mejor accruacy que usando otras técnicas de cuantización como el 'Post-Training Quantization'.\n",
        "\n",
        "Para ello, tenemos que añadir unos adaptadores a la entrada y salida del modelo para convertir las entradas de FP32 a INT8 y nuestras salidas de INT8 a FP32. Tras esto, definimos la librería que realizará la cuantización y que depende del hardware en el que vamos a desplegar. Pytorch ofrece las siguientes opciones: https://pytorch.org/docs/stable/quantization.html#backend-hardware-support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIX9cfdoLPF4",
        "outputId": "77a87ae3-dd0f-41f8-9bb8-2a5e5981c6e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==============================================================================================================\n",
              "Layer (type:depth-idx)                                       Output Shape              Param #\n",
              "==============================================================================================================\n",
              "Sequential                                                   [1, 1000]                 --\n",
              "├─QuantStub: 1-1                                             [1, 3, 224, 224]          --\n",
              "│    └─FusedMovingAvgObsFakeQuantize: 2-1                    [1, 3, 224, 224]          --\n",
              "├─AlexNet: 1-2                                               [1, 1000]                 --\n",
              "│    └─Sequential: 2-2                                       [1, 256, 6, 6]            --\n",
              "│    │    └─Conv2d: 3-1                                      [1, 64, 55, 55]           23,296\n",
              "│    │    └─ReLU: 3-2                                        [1, 64, 55, 55]           --\n",
              "│    │    └─MaxPool2d: 3-3                                   [1, 64, 27, 27]           --\n",
              "│    │    └─Conv2d: 3-4                                      [1, 192, 27, 27]          307,392\n",
              "│    │    └─ReLU: 3-5                                        [1, 192, 27, 27]          --\n",
              "│    │    └─MaxPool2d: 3-6                                   [1, 192, 13, 13]          --\n",
              "│    │    └─Conv2d: 3-7                                      [1, 384, 13, 13]          663,936\n",
              "│    │    └─ReLU: 3-8                                        [1, 384, 13, 13]          --\n",
              "│    │    └─Conv2d: 3-9                                      [1, 256, 13, 13]          884,992\n",
              "│    │    └─ReLU: 3-10                                       [1, 256, 13, 13]          --\n",
              "│    │    └─Conv2d: 3-11                                     [1, 256, 13, 13]          590,080\n",
              "│    │    └─ReLU: 3-12                                       [1, 256, 13, 13]          --\n",
              "│    │    └─MaxPool2d: 3-13                                  [1, 256, 6, 6]            --\n",
              "│    └─AdaptiveAvgPool2d: 2-3                                [1, 256, 6, 6]            --\n",
              "│    └─Sequential: 2-4                                       [1, 1000]                 --\n",
              "│    │    └─Dropout: 3-14                                    [1, 9216]                 --\n",
              "│    │    └─Linear: 3-15                                     [1, 4096]                 37,752,832\n",
              "│    │    └─ReLU: 3-16                                       [1, 4096]                 --\n",
              "│    │    └─Dropout: 3-17                                    [1, 4096]                 --\n",
              "│    │    └─Linear: 3-18                                     [1, 4096]                 16,781,312\n",
              "│    │    └─ReLU: 3-19                                       [1, 4096]                 --\n",
              "│    │    └─Linear: 3-20                                     [1, 1000]                 4,097,000\n",
              "├─DeQuantStub: 1-3                                           [1, 1000]                 --\n",
              "==============================================================================================================\n",
              "Total params: 61,100,840\n",
              "Trainable params: 61,100,840\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0\n",
              "==============================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.60\n",
              "=============================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "model_fp32 = torch.nn.Sequential(torch.quantization.QuantStub(), model, torch.quantization.DeQuantStub())\n",
        "model_fp32.eval()\n",
        "model_fp32.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')\n",
        "model_fp32_prepared = torch.quantization.prepare_qat(model_fp32.train())\n",
        "summary(model_fp32_prepared, input_size=(1, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPFQTzjQQo7Q"
      },
      "source": [
        "## 5. Entrenamiento del modelo\n",
        "\n",
        "Realizamos unas épocas para calibrar los pesos del modelo y adaptarlo a la nueva representación. Para ello, usamos la base de datos que hemos descargado en el punto 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzVUI780Mx8s",
        "outputId": "ad9336c8-b3fe-44e0-d3f4-f40a4a7362da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Summary for epoch 0: loss: 1.02e+07, acc: 10.0]  time: 137.871s **\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 1\n",
        "opt = torch.optim.Adam(model_fp32_prepared.parameters(), lr=0.001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "model_fp32_prepared.train()\n",
        "for epoch in range(n_epochs): # Entrenamos n epocas\n",
        "    train_running_loss = 0.0\n",
        "    train_running_correct = 0\n",
        "    counter = 0\n",
        "    time_start = time.time()\n",
        "    for inputs, labels in train_data_loader: # Obtenemos todos los batch de entrenamiento y los usamos para entrenar\n",
        "        inputs = inputs.to('cuda')\n",
        "        labels = labels.to('cuda')\n",
        "        opt.zero_grad()\n",
        "        outs = model_fp32_prepared(inputs)\n",
        "        loss = loss_fn(outs, labels)\n",
        "        train_running_loss += loss.item()\n",
        "        _, preds = torch.max(outs.data, 1)\n",
        "        train_running_correct += (preds == labels).sum().item()\n",
        "        counter = counter + 1\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    epoch_loss = train_running_loss / counter\n",
        "    epoch_acc = 100. * (train_running_correct / len(train_data_loader.dataset))\n",
        "    time_end = time.time() - time_start\n",
        "    print(f'** Summary for epoch {epoch}: '\n",
        "\t\tf'loss: {epoch_loss:#.3g}, acc: {epoch_acc:#.3g}]  '\n",
        "\t\tf'time: {time_end:.3f}s **')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxGT16QHQ6a0"
      },
      "source": [
        "## 6. Exportar el modelo en INT8\n",
        "\n",
        "Una vez que hemos realizado el entrenamiento para pasar a INT8, simplemente limpiamos las capas auxiliares que añade Pytorch para realizar la calibración y exportamos el modelo a TorchScript para poder usarlo en un móvil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JTIDTKCLidSO"
      },
      "outputs": [],
      "source": [
        "model_fp32_prepared.eval().cpu()\n",
        "model_int8 = torch.quantization.convert(model_fp32_prepared, inplace=True).cpu()\n",
        "model_int8_script = torch.jit.script(model_int8) # Export to TorchScript\n",
        "torch.jit.save(model_int8_script, './model_int8.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcIiFXF7ROfO"
      },
      "source": [
        "Además, vamos a realizar una  inferencia de prueba para analizar el rendimiento del modelo inicial y el cuantizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CNE3EyU6JUx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c70cfbe-7f2d-4fd9-812d-04bbf73352d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 224, 244])\n",
            "Execution time of the fp32 model: 0.129s\n",
            "Execution time of the int8 model: 0.035s\n"
          ]
        }
      ],
      "source": [
        "image = torch.Tensor(np.random.rand(1,3,224,244)).float().cpu()\n",
        "print(image.shape)\n",
        "\n",
        "# FP32\n",
        "model.cpu()\n",
        "time_start = time.time()\n",
        "model(image)\n",
        "time_end = time.time() - time_start\n",
        "print(f'Execution time of the fp32 model: {time_end:.3f}s')\n",
        "\n",
        "# INT8\n",
        "time_start = time.time()\n",
        "model_int8(image)\n",
        "time_end = time.time() - time_start\n",
        "print(f'Execution time of the int8 model: {time_end:.3f}s')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
